{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e75c873c-2c1b-4129-9803-70be2d27b09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import DistilBertConfig, DistilBertTokenizerFast,  DistilBertForSequenceClassification,  PreTrainedTokenizerFast\n",
    "from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e3964a8-3126-4e7d-b73b-188f014a19a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='/lustre/isaac/proj/UTK0196/deep-surface-protein-data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae07435b-88b2-498b-92fa-35fd9d5cb43c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Tells the model we need to use the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55ac6e1d-2e94-484e-b9f4-b4dd809e1a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH+'M0059E_training_set.tsv', delimiter=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5cc48288-e2a4-4068-b2a8-cab4e2820df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2956318/1488259387.py:4: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  classification_df = pd.DataFrame({'text' : surf_series.append(deep_series, ignore_index=True), 'label' : [0]*surf_series.size+[1]*deep_series.size})\n"
     ]
    }
   ],
   "source": [
    "surf_series = df['surf.sequence']\n",
    "deep_series = df['deep.sequence']\n",
    "\n",
    "classification_df = pd.DataFrame({'text' : surf_series.append(deep_series, ignore_index=True), 'label' : [0]*surf_series.size+[1]*deep_series.size})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc566dbc-a5b0-4b6b-8593-2bca9094f7cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def overlap_sequence(seq, word_length, overlap):\n",
    "    if overlap >= word_length:\n",
    "        print('Overlap must be less than word length')\n",
    "        return\n",
    "    \n",
    "    for i in range(0, len(seq)-overlap, word_length-overlap):\n",
    "        yield seq[i:i+word_length]\n",
    "        \n",
    "def get_overlap_array(seq, word_length=5, overlap=2):\n",
    "    return np.array(list(overlap_sequence(seq, word_length, overlap)))\n",
    "\n",
    "def get_overlap_string(seq, word_length=2, overlap=0):\n",
    "    return ' '.join(list(overlap_sequence(seq, word_length, overlap)))\n",
    "\n",
    "def compute_metrics(epred):\n",
    "    # Computes metrics from specialized output from huggingface\n",
    "\n",
    "    preds = np.exp(epred[0]) / np.sum(np.exp(epred[0]), axis = 0)\n",
    "    labels = epred[1]\n",
    "\n",
    "    metrics = {}\n",
    "    metrics['auprc'] = average_precision_score(labels, preds[:,1])\n",
    "    metrics['auroc'] = roc_auc_score(labels, preds[:,1])\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8b75b28-1a61-4cae-a19c-687aca1570cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classification_df['text'] = classification_df['text'].transform(get_overlap_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39a23392-038c-4dbb-ab6f-9de3f7ff1d41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = Dataset.from_pandas(classification_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48a651e1-c023-4953-a19f-7ed9b063954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36661fd4-0acc-451a-a274-97d6d9050e90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7133d4b-cd3b-4a9f-a9bc-cd37fa74fca0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b232203c-a0c9-4067-9371-bd9cb33f9b38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.mask_token=\"[MASK]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e7a812b-8320-4808-9a59-c62be4731a9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee2c2969-e410-45e6-85ca-7acaa36ee4e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = trainers.WordPieceTrainer(special_tokens=special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a14daa76-6a51-4fa0-ab4f-5004f7a85813",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "all_texts = [ds[i : i + batch_size]['text'] for i in range(0, len(ds), batch_size)]\n",
    "\n",
    "def batch_iterator():\n",
    "    for i in range(0, len(ds), batch_size):\n",
    "        yield ds[i : i + batch_size]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a275d727-9603-4529-bc98-d2a5904c507d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ba55e692-fe9d-49c2-8d5d-ee13235c205b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9de43b58-e85c-4b66-b92d-54785b007ac7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fast_tokenizer.mask_token='[MASK]'\n",
    "fast_tokenizer.pad_token='[PAD]'\n",
    "fast_tokenizer.cls_token='[CLS]'\n",
    "fast_tokenizer.unk_token='[UNK]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46059fd0-347d-41e4-9dba-7c4aed179570",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizers/AA-pairs/tokenizer_config.json',\n",
       " 'tokenizers/AA-pairs/special_tokens_map.json',\n",
       " 'tokenizers/AA-pairs/tokenizer.json')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer.save_pretrained('tokenizers/AA-pairs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4d1ab463-5c32-455d-8c2a-887fac792a9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#testing it\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('tokenizers/AA-pairs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a10d6e25-1151-436a-bb9d-7cc434560cf0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/460912 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "tokenized_ds = ds.map(lambda d : tokenizer(d['text'], truncation=True), batched=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
